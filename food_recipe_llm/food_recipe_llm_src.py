# -*- coding: utf-8 -*-
"""food_recipe_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUb5Bw3w-gWdwAtQjkQ0uGRRgVFM9b-V

#S0: Data Source and Installing Packages
"""

#2.2M recipes from kaggle

# https://www.kaggle.com/datasets/paultimothymooney/recipenlg

#install dependencies

#pip = package installer for python, -q is quiet mode to suppress most dialogue

# !pip -q install transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu

#colab forgets about these so just reinstall each time
!pip -q install faiss-cpu
!pip -q install langchain-community
!pip -q install -U langchain-huggingface

"""#S1: Loading Model"""

#this cell is to silence the loading messages and to suppress the widgets when loading in the model

import os, warnings, logging as pylog, io, contextlib
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import logging as tf_logging
from huggingface_hub.utils import logging as hub_logging


os.environ["HF_HUB_DISABLE_PROGRESS_BAR"] = "1"
os.environ["TQDM_DISABLE"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

tf_logging.set_verbosity_error()
hub_logging.set_verbosity_error()
pylog.getLogger("transformers").setLevel(pylog.ERROR)
pylog.getLogger("huggingface_hub").setLevel(pylog.ERROR)
warnings.filterwarnings("ignore")

model_name = "microsoft/Phi-3-mini-128k-instruct"

buf = io.StringIO()
with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):
    tok = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )

print("Model and tokenizer loaded successfully")
print(f"Running on {model.device}")

"""#S2: Embeddings for Document Chunking"""

from langchain_huggingface import HuggingFaceEmbeddings

#to turn user queries into vectors as well as document chunks later, which are stored in vector db
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

#example to verify it works
# print(embeddings.embed_query("Give me the recipe for a pizza"))

"""#S3: Accessing Data from Drive

"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

#dataset of 2.2M recipes, ingredients (with named entity recognition), and directions
path = "/content/drive/My Drive/recipes.csv"

#just take the first 100k/2M rows so as to speed up the process of loading
df = pd.read_csv(path, nrows=100000)

#to verify it works
# print(df.shape)

"""#S4: Data Cleaning to Feed to Model

"""

#list columns for combine_recipe function, to see what is relevant
# print(df.columns)

#combining all info to embed and store to contain all relevant information for llm retrieval
def combine_recipe(row):
  info_per_row = f"Title: {row['title']}\nIngredients: {row['ingredients']}\nDirections: {row['directions']}"
  return info_per_row

#axis=1 = apply function to each row
df['info'] = df.apply(combine_recipe, axis=1)

#to verify it works
# print(df['info'][0]

#take only df[info] and convert to list for langchain's faiss, as it expects a list of strings
docs = df['info'].tolist()

#to verify it works
# print(texts[0])

"""#S5: Vector Database"""

from langchain.vectorstores import FAISS

#its normal for this part to take a while to load

#takes each string in the "docs" list (each row of df['info']),
#turns each string into a vector embedding using the embedding model taken earlier, stores inside a faiss index
#user query is also vectorized, and the closest recipe representation in the vector space is taken as context for the llm response

db = FAISS.from_texts(texts=docs, embedding=embeddings)

#saves so re-embedding each time is not need, time efficient
db.save_local("faiss_index")
db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

"""#S6: K-Nearest Vectors and Pipeline Structure"""

from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

#to put model in a langchain wrapper
from transformers import pipeline

#returns "k" vectors to query, here it retrieves 3 closest document chunk vectors, uses all as context for llm response
retriever = db.as_retriever(search_kwargs={"k":3})

#usually a device= arg in pipeline() but interferes here as i set gpu to auto earlier in pipeline so just uses that
#device=0 means use first gpu
#device=1 means use second gpu
#device=-1 means cpu

#tells the model its task, to generate text, and feeds in the neccessary information to do it

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tok,
    max_new_tokens=512,
    temperature=0.1, #creative but not too much, so it doesnt just riff about nonsense
    top_p=0.9,
    repetition_penalty=1.1,
    eos_token_id=tok.eos_token_id
)

#pipe is a huggingface object and this wraps it in langchain's llm interface
#so now langchain can call invoke(), .predict() or use inside chains like RetrievalQA
#wrapping is neccessary because hf pipelines return generated text in a list-like format and langchain expects plain strings, so the wrapper just converts it

llm = HuggingFacePipeline(pipeline=pipe)

"""#S7: System Instructions and Prompt Template"""

from langchain.prompts import PromptTemplate

template = """
You are a recipe assistant.
Output the recipe in this format:

Title: <title>
Ingredients:
- ...
Directions:
1. ...
2. ...
(END OF RECIPE)

Recipes:
{context}

Question: {question}

Answer:
"""

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False,
    chain_type_kwargs={"prompt": custom_prompt})

"""#S8: Interactive Display

the user can submit queries for the chatbot by running this cell!
"""

while True:

  #preferred that the user asks about food to test the implementation of RAG, but one can ask about anything
  query = input("Query: ")
  result = qa.invoke({"query": query})

  raw_output = result["result"]

  if "Answer:" in raw_output:
    answer = raw_output.split("Answer:")[-1].strip()
  else:
    answer = raw_output.strip()

  #result is a dictionary with keys named query, result, and source_documents
  print("\nAnswer:", answer, "\n")