{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#S0: Data Source and Installing Packages"
      ],
      "metadata": {
        "id": "mRQMp10sGPvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2M recipes from kaggle\n",
        "\n",
        "# https://www.kaggle.com/datasets/paultimothymooney/recipenlg"
      ],
      "metadata": {
        "id": "UUUMEFByGTjK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mofbx2SOO0E4"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "\n",
        "#pip = package installer for python, -q is quiet mode to suppress most dialogue\n",
        "\n",
        "# !pip -q install transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu\n",
        "\n",
        "#colab forgets about these so just reinstall each time\n",
        "!pip -q install faiss-cpu\n",
        "!pip -q install langchain-community\n",
        "!pip -q install -U langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S1: Loading Model"
      ],
      "metadata": {
        "id": "3XsaaZVDGlO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this cell is to silence the loading messages and to suppress the widgets when loading in the model\n",
        "\n",
        "import os, warnings, logging as pylog, io, contextlib\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import logging as tf_logging\n",
        "from huggingface_hub.utils import logging as hub_logging\n",
        "\n",
        "\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BAR\"] = \"1\"\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "tf_logging.set_verbosity_error()\n",
        "hub_logging.set_verbosity_error()\n",
        "pylog.getLogger(\"transformers\").setLevel(pylog.ERROR)\n",
        "pylog.getLogger(\"huggingface_hub\").setLevel(pylog.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "buf = io.StringIO()\n",
        "with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully\")\n",
        "print(f\"Running on {model.device}\")\n"
      ],
      "metadata": {
        "id": "x_LTZd5YOQV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "9d0a63d35757413ba6f491dfd75cd256",
            "63be201c01be4b3494d8e0b5c5b5fc6f",
            "2b535b422b8741aaaa08c100e2bc6193",
            "a92fbf83f4574cc29293031dd30240bd",
            "4180987da68940189a6cb412cbd134b4",
            "1b19c204ca1046a3bd997039dbac6fd2",
            "c773966b4c43496c85f34a07fff7d663",
            "2536bb72396d466b9e504f0810ac3453",
            "3cc7ca8d14af4e999d0cb04bcc910f28",
            "9cb529d18aaf4e8990c2e80f192f92bd",
            "a9368c3a9a574c13b9c740f41547f3df"
          ]
        },
        "outputId": "5053c4bd-2572-4cce-a1c7-187a79cc038b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d0a63d35757413ba6f491dfd75cd256"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully\n",
            "Running on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S2: Embeddings for Document Chunking"
      ],
      "metadata": {
        "id": "dJtb71wpGs_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "#to turn user queries into vectors as well as document chunks later, which are stored in vector db\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "#example to verify it works\n",
        "# print(embeddings.embed_query(\"Give me the recipe for a pizza\"))"
      ],
      "metadata": {
        "id": "veA0AmeLGsD5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S3: Accessing Data from Drive\n"
      ],
      "metadata": {
        "id": "aoOKVDwNG3PB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7WbPvnzZnX",
        "outputId": "2c4d9b1d-90be-4b90-bd56-ce0ab95bd860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#dataset of 2.2M recipes, ingredients (with named entity recognition), and directions\n",
        "path = \"/content/drive/My Drive/recipes.csv\"\n",
        "\n",
        "#just take the first 100k/2M rows so as to speed up the process of loading\n",
        "df = pd.read_csv(path, nrows=100000)\n",
        "\n",
        "#to verify it works\n",
        "# print(df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S4: Data Cleaning to Feed to Model\n"
      ],
      "metadata": {
        "id": "s0i8V6oKG61S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C2t1X4kb9Orl"
      },
      "outputs": [],
      "source": [
        "#list columns for combine_recipe function, to see what is relevant\n",
        "# print(df.columns)\n",
        "\n",
        "#combining all info to embed and store to contain all relevant information for llm retrieval\n",
        "def combine_recipe(row):\n",
        "  info_per_row = f\"Title: {row['title']}\\nIngredients: {row['ingredients']}\\nDirections: {row['directions']}\"\n",
        "  return info_per_row\n",
        "\n",
        "#axis=1 = apply function to each row\n",
        "df['info'] = df.apply(combine_recipe, axis=1)\n",
        "\n",
        "#to verify it works\n",
        "# print(df['info'][0]\n",
        "\n",
        "#take only df[info] and convert to list for langchain's faiss, as it expects a list of strings\n",
        "docs = df['info'].tolist()\n",
        "\n",
        "#to verify it works\n",
        "# print(texts[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S5: Vector Database"
      ],
      "metadata": {
        "id": "ERhuymPqHOyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "#its normal for this part to take a while to load\n",
        "\n",
        "#takes each string in the \"docs\" list (each row of df['info']),\n",
        "#turns each string into a vector embedding using the embedding model taken earlier, stores inside a faiss index\n",
        "#user query is also vectorized, and the closest recipe representation in the vector space is taken as context for the llm response\n",
        "\n",
        "db = FAISS.from_texts(texts=docs, embedding=embeddings)\n",
        "\n",
        "#saves so re-embedding each time is not need, time efficient\n",
        "db.save_local(\"faiss_index\")\n",
        "db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n"
      ],
      "metadata": {
        "id": "Ga2FvwdfHAA3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S6: K-Nearest Vectors and Pipeline Structure"
      ],
      "metadata": {
        "id": "vgaMC169HVn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "#to put model in a langchain wrapper\n",
        "from transformers import pipeline\n",
        "\n",
        "#returns \"k\" vectors to query, here it retrieves 3 closest document chunk vectors, uses all as context for llm response\n",
        "retriever = db.as_retriever(search_kwargs={\"k\":3})\n",
        "\n",
        "#usually a device= arg in pipeline() but interferes here as i set gpu to auto earlier in pipeline so just uses that\n",
        "#device=0 means use first gpu\n",
        "#device=1 means use second gpu\n",
        "#device=-1 means cpu\n",
        "\n",
        "#tells the model its task, to generate text, and feeds in the neccessary information to do it\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tok,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1, #creative but not too much, so it doesnt just riff about nonsense\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        "    eos_token_id=tok.eos_token_id\n",
        ")\n",
        "\n",
        "#pipe is a huggingface object and this wraps it in langchain's llm interface\n",
        "#so now langchain can call invoke(), .predict() or use inside chains like RetrievalQA\n",
        "#wrapping is neccessary because hf pipelines return generated text in a list-like format and langchain expects plain strings, so the wrapper just converts it\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n"
      ],
      "metadata": {
        "id": "nZCbA5GOHAoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14523b95-152c-44ed-8aa0-f3d4e52dbbf5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-110940160.py:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S7: System Instructions and Prompt Template"
      ],
      "metadata": {
        "id": "JgucEpvLHboF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FXlnoop_7yjy"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are a recipe assistant.\n",
        "Output the recipe in this format:\n",
        "\n",
        "Title: <title>\n",
        "Ingredients:\n",
        "- ...\n",
        "Directions:\n",
        "1. ...\n",
        "2. ...\n",
        "(END OF RECIPE)\n",
        "\n",
        "Recipes:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=False,\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S8: Interactive Display"
      ],
      "metadata": {
        "id": "lyBVZHBEHgiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the user can submit queries for the chatbot by running this cell!"
      ],
      "metadata": {
        "id": "K-Clm8i_K1xz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BpQ9BiPS1Bkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a5a2276-eddb-4e7d-9286-4223506e5a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: pizza recipe\n",
            "\n",
            "Answer: Title: Classic Margherita Pizza\n",
            "Ingredients:\n",
            "- 2 pkg. pizza dough mix\n",
            "- 1 jar marinara sauce\n",
            "- Fresh basil leaves\n",
            "- 1 ball buffalo mozzarella cheese\n",
            "- Extra virgin olive oil\n",
            "- Salt\n",
            "\n",
            "Directions:\n",
            "1. Preheat your oven to its highest setting, typically around 475°F (246°C). If you have a pizza stone, put it in the oven now too.\n",
            "2. On a lightly floured surface, stretch or roll out one package of pizza dough into an approximately 12-inch circle. Transfer the dough onto a piece of parchment paper if using a baking sheet; otherwise, transfer directly onto a preheated pizza stone.\n",
            "3. Spread half of the marinara sauce evenly across the base of the pizza, leaving a border around the edges free from sauce.\n",
            "4. Slice the buffalo mozzarella into thin rounds and distribute them evenly over the sauce. Add some torn fresh basil leaves among the tomato sauce and cheese. Drizzle extra virgin olive oil generously over everything. Season with a pinch of salt.\n",
            "5. Carefully slide the pizza (with the parchment underneath if used) onto the middle rack of the oven. Bake for about 10-12 minutes, or until the crust is golden and puffy and the cheese has started to bubble and turn slightly golden.\n",
            "6. Remove the pizza from the oven carefully – use caution when handling the hot pizza! Let it cool for a few minutes before cutting into wedges and serving immediately while still warm. Enjoy your homemade classic margherita pizza! \n",
            "\n",
            "Query: pasta recipe\n",
            "\n",
            "Answer: Title: Creamy Mushroom Spinach Pasta\n",
            "Ingredients:\n",
            "- 8 oz. bow tie pasta\n",
            "- 2 tablespoons unsalted butter\n",
            "- 1 small shallot, finely chopped\n",
            "- 2 cups fresh spinach leaves, roughly chopped\n",
            "- 8 oz. button mushrooms, thinly sliced\n",
            "- 1 cup heavy cream\n",
            "- Salt and black pepper, to taste\n",
            "- Grated parmesan cheese, for serving\n",
            "\n",
            "Directions:\n",
            "1. Bring a large pot of lightly salted water to a boil. Add the pasta and cook for 8 to 10 minutes or until al dente. Reserve one cup of pasta water before draining. Set aside.\n",
            "2. Meanwhile, in a large skillet, melt the butter over medium heat. Add the shallots and sauté until softened, around 2-3 minutes. Stir in the mushrooms and continue to cook until they release their moisture and begin to brown, approximately 6-7 minutes. Season with salt and pepper as needed.\n",
            "3. Lower the heat slightly then gradually whisk in the heavy cream. Allow it to come to a gentle simmer while continuously stirring. If your sauce is too thick, you may use some reserved pasta water to reach desired consistency. Simmer for another 2-3 minutes.\n",
            "4. Once the sauce has thickened, fold in the chopped spinach and allow it to wilt from the residual heat of the sauce - take care that no excessive liquid remains atop the sauce. Adjust seasoning if necessary.\n",
            "5. Combine the hot pasta with the creamy mushroom spinach sauce ensuring each strand gets coated evenly. Serve immediately garnished generously with grated parmesan cheese. Enjoy! \n",
            "\n",
            "Query: salad recipe\n",
            "\n",
            "Answer: Title: Classic Cobb Salad\n",
            "Ingredients:\n",
            "- 1 head romaine lettuce, washed and chopped\n",
            "- 1 avocado, ripe but firm, peeled, pitted, and cubed\n",
            "- 1 tomato, cored and diced\n",
            "- 1 hard-boiled egg, halved\n",
            "- 1/2 cup blue cheese crumbles\n",
            "- 1/2 cup fresh basil leaves, roughly chopped\n",
            "- Salt and black pepper to taste\n",
            "- Your favorite dressing (optional), e.g., Ranch, Blue Cheese, Balsamic Vinaigrette\n",
            "\n",
            "Directions:\n",
            "1. In a large mixing bowl, combine the chopped romaine lettuce, avocado, tomato, and half of the hard-boiled egg halves. Add salt and black pepper to your liking.\n",
            "2. Sprinkle over the blue cheese crumbles and fresh basil leaves. Gently toss everything together until well combined. If desired, drizzle some of your chosen dressing over the salad just before serving. Serve immediately as it's best enjoyed when chilled. Enjoy!\n",
            "\n",
            "Note: This classic Cobb salad is perfect for any occasion - from casual lunches at home to elegant dinner parties. Feel free to customize according to personal preferences by adding other proteins like grilled chicken breast or turkey sausage, roasted vegetables such as zucchini or sweet potatoes, or even nuts like walnuts or pecans for extra texture and flavor. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3116547867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#preferred that the user asks about food to test the implementation of RAG, but one can ask about anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "\n",
        "  #preferred that the user asks about food to test the implementation of RAG, but one can ask about anything\n",
        "  query = input(\"Query: \")\n",
        "  result = qa.invoke({\"query\": query})\n",
        "\n",
        "  raw_output = result[\"result\"]\n",
        "\n",
        "  if \"Answer:\" in raw_output:\n",
        "    answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "  else:\n",
        "    answer = raw_output.strip()\n",
        "\n",
        "  #result is a dictionary with keys named query, result, and source_documents\n",
        "  print(\"\\nAnswer:\", answer, \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d0a63d35757413ba6f491dfd75cd256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63be201c01be4b3494d8e0b5c5b5fc6f",
              "IPY_MODEL_2b535b422b8741aaaa08c100e2bc6193",
              "IPY_MODEL_a92fbf83f4574cc29293031dd30240bd"
            ],
            "layout": "IPY_MODEL_4180987da68940189a6cb412cbd134b4"
          }
        },
        "63be201c01be4b3494d8e0b5c5b5fc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b19c204ca1046a3bd997039dbac6fd2",
            "placeholder": "​",
            "style": "IPY_MODEL_c773966b4c43496c85f34a07fff7d663",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2b535b422b8741aaaa08c100e2bc6193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2536bb72396d466b9e504f0810ac3453",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cc7ca8d14af4e999d0cb04bcc910f28",
            "value": 2
          }
        },
        "a92fbf83f4574cc29293031dd30240bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cb529d18aaf4e8990c2e80f192f92bd",
            "placeholder": "​",
            "style": "IPY_MODEL_a9368c3a9a574c13b9c740f41547f3df",
            "value": " 2/2 [00:33&lt;00:00, 15.94s/it]"
          }
        },
        "4180987da68940189a6cb412cbd134b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b19c204ca1046a3bd997039dbac6fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c773966b4c43496c85f34a07fff7d663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2536bb72396d466b9e504f0810ac3453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc7ca8d14af4e999d0cb04bcc910f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cb529d18aaf4e8990c2e80f192f92bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9368c3a9a574c13b9c740f41547f3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}