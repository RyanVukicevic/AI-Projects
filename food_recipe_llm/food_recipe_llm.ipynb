{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#S0: Data Source and Installing Packages"
      ],
      "metadata": {
        "id": "mRQMp10sGPvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2M recipes from kaggle\n",
        "\n",
        "# https://www.kaggle.com/datasets/paultimothymooney/recipenlg"
      ],
      "metadata": {
        "id": "UUUMEFByGTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mofbx2SOO0E4"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "\n",
        "#pip = package installer for python, -q is quiet mode to suppress most dialogue\n",
        "\n",
        "# !pip -q install transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu\n",
        "\n",
        "#colab forgets about these so just reinstall each time\n",
        "!pip -q install faiss-cpu\n",
        "!pip -q install langchain-community\n",
        "!pip -q install -U langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S1: Loading Pre-Trained Model"
      ],
      "metadata": {
        "id": "3XsaaZVDGlO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d643139e715c4529b13d650f203096aa",
            "60c04623132243aaa3a57bb8b61ca1d5",
            "5ce3bbd552814e42ad8e28407b3fa79e",
            "9d4297b67b464838bf9ebac1301a2dc3",
            "e3cf0fbb417c48948dab1c6f2b41b515",
            "472891bb4de3404489c6296b38732a3a",
            "94248f345f56454b83d9d7ca7f440e35",
            "555e7a3180f54ce49d9ed53a0fdeaf40",
            "0472cd7aaaec4b449c3f23e4849ae30b",
            "e6c9ff9b2c0b4811bc399441130afe85",
            "35a20c00bd104574a54bcb37d3ca0248"
          ]
        },
        "id": "hjlESyIJEins",
        "outputId": "3fe5d75b-ae92-4252-feca-110d2301a8a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d643139e715c4529b13d650f203096aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully\n",
            "Running on cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#device maps to gpu automatically as gpu > cpu for matrix math, but after usage limits, cpu used\n",
        "\n",
        "#float16 uses half precision instead of full (32) which cuts memory use, speeds inference\n",
        "#most of the time it doesnt matter but for computations it can\n",
        "\n",
        "#its normal for this part to take a while to load\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully\")\n",
        "print(f\"Running on {model.device}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S2: Loading Embeddings for Document Chunking"
      ],
      "metadata": {
        "id": "dJtb71wpGs_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "#to turn user queries into vectors as well as document chunks later, which are stored in vector db\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "#example to verify it works\n",
        "# print(embeddings.embed_query(\"Give me the recipe for a pizza\"))"
      ],
      "metadata": {
        "id": "veA0AmeLGsD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S3: Connecting to Drive to Access Data"
      ],
      "metadata": {
        "id": "aoOKVDwNG3PB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7WbPvnzZnX",
        "outputId": "a9b723ae-a970-4031-a071-58d9516ecd8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#dataset of 2.2M recipes, ingredients (with named entity recognition), and directions\n",
        "path = \"/content/drive/My Drive/recipes.csv\"\n",
        "\n",
        "#just take the first 100k/2M rows so as to speed up the process of loading\n",
        "df = pd.read_csv(path, nrows=100000)\n",
        "\n",
        "#to verify it works\n",
        "# print(df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S4: Data Cleaning to Feed to Model\n"
      ],
      "metadata": {
        "id": "s0i8V6oKG61S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C2t1X4kb9Orl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27fe819b-be88-45a9-ad3b-f37cec36a3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/tmp/ipython-input-2899294009.py:67: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ],
      "source": [
        "#list columns for combine_recipe function, to see what is relevant\n",
        "# print(df.columns)\n",
        "\n",
        "#combining all info to embed and store to contain all relevant information for llm retrieval\n",
        "def combine_recipe(row):\n",
        "  info_per_row = f\"Title: {row['title']}\\nIngredients: {row['ingredients']}\\nDirections: {row['directions']}\"\n",
        "  return info_per_row\n",
        "\n",
        "#axis=1 = apply function to each row\n",
        "df['info'] = df.apply(combine_recipe, axis=1)\n",
        "\n",
        "#to verify it works\n",
        "# print(df['info'][0]\n",
        "\n",
        "#take only df[info] and convert to list for langchain's faiss, as it expects a list of strings\n",
        "docs = df['info'].tolist()\n",
        "\n",
        "#to verify it works\n",
        "print(texts[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S5: Vector Database"
      ],
      "metadata": {
        "id": "ERhuymPqHOyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "#its normal for this part to take a while to load\n",
        "\n",
        "#takes each string in the \"docs\" list (each row of df['info']),\n",
        "#turns each string into a vector embedding using the embedding model taken earlier, stores inside a faiss index\n",
        "#user query is also vectorized, and the closest recipe representation in the vector space is taken as context for the llm response\n",
        "\n",
        "db = FAISS.from_texts(texts=docs, embedding=embeddings)\n",
        "\n",
        "#saves so re-embedding each time is not need, time efficient\n",
        "db.save_local(\"faiss_index\")\n",
        "db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n"
      ],
      "metadata": {
        "id": "Ga2FvwdfHAA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S6: Model Settings and Langchain/HF Wrapping"
      ],
      "metadata": {
        "id": "vgaMC169HVn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "#to put model in a langchain wrapper\n",
        "from transformers import pipeline\n",
        "\n",
        "#returns \"k\" vectors to query, here it retrieves 3 closest document chunk vectors, uses all as context for llm response\n",
        "retriever = db.as_retriever(search_kwargs={\"k\":3})\n",
        "\n",
        "#usually a device= arg in pipeline() but interferes here as i set gpu to auto earlier in pipeline so just uses that\n",
        "#device=0 means use first gpu\n",
        "#device=1 means use second gpu\n",
        "#device=-1 means cpu\n",
        "\n",
        "#tells the model its task, to generate text, and feeds in the neccessary information to do it\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tok,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1, #creative but not too much, so it doesnt just riff about nonsense\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        "    eos_token_id=tok.eos_token_id\n",
        ")\n",
        "\n",
        "#pipe is a huggingface object and this wraps it in langchain's llm interface\n",
        "#so now langchain can call invoke(), .predict() or use inside chains like RetrievalQA\n",
        "#wrapping is neccessary because hf pipelines return generated text in a list-like format and langchain expects plain strings, so the wrapper just converts it\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "# llm.pipeline_kwargs = {\"stop\": [\"(END OF RECIPE)\"]}"
      ],
      "metadata": {
        "id": "nZCbA5GOHAoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S7: System Instructions for LLM"
      ],
      "metadata": {
        "id": "JgucEpvLHboF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FXlnoop_7yjy"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are a recipe assistant.\n",
        "Output the recipe in this format:\n",
        "\n",
        "Title: <title>\n",
        "Ingredients:\n",
        "- ...\n",
        "Directions:\n",
        "1. ...\n",
        "2. ...\n",
        "(END OF RECIPE)\n",
        "\n",
        "Recipes:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=False,\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#S8: Interactive Display"
      ],
      "metadata": {
        "id": "lyBVZHBEHgiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpQ9BiPS1Bkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8708c00-9e5c-4a55-e109-9d52bd97a7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: pizza recipe\n",
            "\n",
            "Answer: Title: Classic Margherita Pizza\n",
            "Ingredients:\n",
            "- 2 pkg. pizza dough mix\n",
            "- 1 jar marinara sauce\n",
            "- Fresh basil leaves\n",
            "- 1 ball buffalo mozzarella cheese\n",
            "- Extra virgin olive oil\n",
            "- Salt to taste\n",
            "\n",
            "Directions:\n",
            "1. Preheat your oven to its highest setting, typically around 475°F (246°C). If you have a pizza stone, put it in the oven now too.\n",
            "2. On a lightly floured surface, stretch or roll out one package of pizza dough into an approximately 12-inch circle. Transfer the dough onto a piece of parchment paper if using a baking sheet; otherwise, transfer directly onto a preheated pizza stone.\n",
            "3. Spread half of the marinara sauce evenly across the base of the pizza, leaving a border for the crust edge.\n",
            "4. Slice the buffalo mozzarella into thin rounds and distribute them over the sauce. Add some torn fresh basil leaves along with a drizzle of extra virgin olive oil. Season with a pinch of salt.\n",
            "5. Carefully slide the pizza (with the parchment paper underneath if used) onto the middle rack of the oven. Bake for about 10-12 minutes, or until the edges are golden and the cheese has started to bubble and slightly char.\n",
            "6. Remove from the oven and let cool for a few minutes before cutting into wedges. Serve immediately while still warm. Enjoy! \n",
            "\n",
            "Query: pasta recipe\n",
            "\n",
            "Answer: Title: Creamy Mushroom Spinach Pasta\n",
            "Ingredients:\n",
            "- 8 oz. bow tie pasta\n",
            "- 1 tablespoon unsalted butter\n",
            "- 1 cup finely chopped shallots\n",
            "- 6 cups baby spinach leaves\n",
            "- Salt and black pepper, to taste\n",
            "- 1 lb. button mushrooms, cleaned and thinly sliced\n",
            "- 1/2 cup dry white wine\n",
            "- 1 cup heavy cream\n",
            "- 1/4 cup grated Parmesan cheese\n",
            "- Fresh parsley, chopped (for garnish)\n",
            "\n",
            "Directions:\n",
            "1. Bring a large pot of lightly salted water to a boil. Add the pasta and cook as per package instructions until al dente. Reserve one cup of pasta water before draining. Set aside.\n",
            "2. In a large sauté pan, melt the butter over medium heat. Add the shallots and cook until softened, approximately 3-4 minutes. Stir occasionally.\n",
            "3. Add the spinach to the pan and season with salt and pepper. Continue to cook while constantly tossing until wilted, which should take around 2-3 minutes. Remove from heat once done.\n",
            "4. While the spinach is wilting, place your sliced mushrooms onto another pan set at medium high heat. Sauté them without any additional liquid until they're browned and have released their moisture - roughly 5-7 minutes. Season with some salt here too if needed. Once ready, remove from heat.\n",
            "5. Pour off excess fat from both pans leaving behind only residual flavorful bits stuck to bottoms – these will enhance our final sauce! Return each pan back up to low heat separately now.\n",
            "6. To the same pan where you had been cooking the mushrooms, deglaze it by adding the white wine then let it simmer down till reduced by half. This process helps extract maximum flavor out of those delicious brown bits left behind earlier. Then whisk in the heavy cream followed immediately after that with the reserved pasta water gradually incorporating all along mixing continuously throughout every step ensuring smooth consistency avoiding lumps formation within sauce itself later during serving time ahead when plating portions individually served hot directly \n",
            "\n",
            "Query: salad recipe\n",
            "\n",
            "Answer: Title: Classic Garden Salad\n",
            "Ingredients:\n",
            "- 1 head romaine lettuce, washed and chopped\n",
            "- 1 medium tomato, diced\n",
            "- 1 cucumber, seeded and sliced\n",
            "- 1/2 red onion, thinly sliced\n",
            "- 1 carrot, julienned\n",
            "- 1/4 cup walnuts, toasted and roughly chopped\n",
            "- Salt and freshly ground black pepper, to taste\n",
            "- Your favorite vinaigrette dressing\n",
            "\n",
            "Directions:\n",
            "1. In a large salad bowl, combine the chopped romaine lettuce, diced tomato, sliced cucumber, thinly sliced red onion, julienned carrot, and toasted walnuts.\n",
            "2. Season the salad mixture with salt and freshly ground black pepper according to your preference.\n",
            "3. Drizzle your desired amount of vinaigrette dressing over the salad just before serving. Gently toss everything together until well coated. Serve immediately as a refreshing side dish or main course! Enjoy! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "\n",
        "  #preferred that the user asks about food to test the implementation of RAG, but one can ask about anything\n",
        "  query = input(\"Query: \")\n",
        "  result = qa.invoke({\"query\": query})\n",
        "\n",
        "  raw_output = result[\"result\"]\n",
        "\n",
        "  if \"Answer:\" in raw_output:\n",
        "    answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "  else:\n",
        "    answer = raw_output.strip()\n",
        "\n",
        "  #result is a dictionary with keys named query, result, and source_documents\n",
        "  print(\"\\nAnswer:\", answer, \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d643139e715c4529b13d650f203096aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60c04623132243aaa3a57bb8b61ca1d5",
              "IPY_MODEL_5ce3bbd552814e42ad8e28407b3fa79e",
              "IPY_MODEL_9d4297b67b464838bf9ebac1301a2dc3"
            ],
            "layout": "IPY_MODEL_e3cf0fbb417c48948dab1c6f2b41b515"
          }
        },
        "60c04623132243aaa3a57bb8b61ca1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472891bb4de3404489c6296b38732a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_94248f345f56454b83d9d7ca7f440e35",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5ce3bbd552814e42ad8e28407b3fa79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_555e7a3180f54ce49d9ed53a0fdeaf40",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0472cd7aaaec4b449c3f23e4849ae30b",
            "value": 2
          }
        },
        "9d4297b67b464838bf9ebac1301a2dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c9ff9b2c0b4811bc399441130afe85",
            "placeholder": "​",
            "style": "IPY_MODEL_35a20c00bd104574a54bcb37d3ca0248",
            "value": " 2/2 [00:40&lt;00:00, 19.54s/it]"
          }
        },
        "e3cf0fbb417c48948dab1c6f2b41b515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472891bb4de3404489c6296b38732a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94248f345f56454b83d9d7ca7f440e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "555e7a3180f54ce49d9ed53a0fdeaf40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0472cd7aaaec4b449c3f23e4849ae30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6c9ff9b2c0b4811bc399441130afe85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a20c00bd104574a54bcb37d3ca0248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}